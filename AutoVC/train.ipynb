{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! apt-get install -y portaudio19-dev \n","! apt-get install python3-all-dev\n","\n","! pip install librosa tqdm gdown pyaudio wave pydub noisereduce"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26105,"status":"ok","timestamp":1683369252459,"user":{"displayName":"吳宣逸","userId":"18311614325200658017"},"user_tz":-480},"id":"M8LgeLEju3R-","outputId":"bbd91974-2c45-4336-a29c-8793234df5d2"},"outputs":[],"source":["! gdown 1Qq4WdRhAT2GNCdGcSpGLCpA21Uy-N3hc # my training data\n","! unzip -qq spmel.zip\n","! rm spmel.zip"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lNOhO5xNu3SB"},"source":["# Preprocess\n","### Prepare the training data\n","* Put wav files in `/wavs` folder.\n","* Please arrange the wav files for each speakers as the following:\n","    ```\n","    wavs\n","    ├── speaker1\n","    |   ├── speaker1_001.wav\n","    |   └── speaker1_002.wav\n","    ├── speaker2\n","    |   ├── speaker2_001.wav\n","    |   └── speaker2_002.wav\n","    ├── ...\n","    ```\n","* It's better to use same number of utterances per speaker.\n","* I use the subset of [VCTK corpus](https://datashare.ed.ac.uk/handle/10283/2950) as the raw training data.\n","\n","### Convert to mel spectrums\n","* Use the command: ```python make_spec.py --rootDir \"./wavs\" --targetDir \"./spmel\"```\n","* My training dataset is [here](https://drive.google.com/file/d/1Qq4WdRhAT2GNCdGcSpGLCpA21Uy-N3hc/view?usp=drive_link).\n","\n","### Extract the feature of speaker\n","* Use the command: ```! python make_d_vector.py --num_uttrs 400 --rootDir \"./spmel\" --model \"./3000000-BL.ckpt\"```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ! python make_spec.py --rootDir \"./wavs\" --targetDir \"./spmel\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import os\n","# from random import shuffle\n","\n","# num_uttrs = 400\n","# rootdir = './spmel'\n","\n","# dirs = []\n","# dirpath, dirnames, filenames = next(os.walk(rootdir))\n","# for dir in dirnames:\n","#     path = os.path.join(dirpath, dir)\n","#     dirs.append(path)\n","#     print(path)\n","#     num_uttrs = min(num_uttrs, len(os.listdir(path)))\n","# print(f'num_uttrs: {num_uttrs}')\n","\n","# for dir in dirs:\n","#     left_files = os.listdir(dir)\n","#     shuffle(left_files)\n","#     left_files = left_files[:num_uttrs]\n","#     for f in os.listdir(dir):\n","#         if f not in left_files:\n","#             os.remove(os.path.join(dir,f))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7032,"status":"ok","timestamp":1683369259488,"user":{"displayName":"吳宣逸","userId":"18311614325200658017"},"user_tz":-480},"id":"bBd5sPg4u3SC","outputId":"62811c90-6850-4779-8cc9-a2985cd1624e"},"outputs":[],"source":["! python make_d_vector.py --num_uttrs 400 --rootDir \"./spmel\" --model \"./3000000-BL.ckpt\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"748Oo5V1u3SC"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from solver_encoder import Solver\n","from data_loader import get_loader\n","\n","class Config:\n","    def __init__(self):\n","        self.data_dir = './spmel'\n","        self.batch_size = 20\n","        self.len_crop = 176\n","        self.lambda_cd = 1\n","        self.dim_neck = 32\n","        self.dim_emb = 256\n","        self.dim_pre = 512\n","        self.freq = 22\n","        self.num_iters = 1000000\n","        self.log_step = 1000\n","        self.early_stop = 20000\n","config = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4i_p9E_eu3SC","outputId":"3464429a-9eda-42d9-e7a5-ce8582cb91ac"},"outputs":[],"source":["vcc_loader = get_loader(config.data_dir, config.batch_size, config.len_crop)\n","solver = Solver(vcc_loader, config, checkpoint='autovc_136000.ckpt') # TODO: load checkpoint\n","solver.train()\n","torch.save(solver.best_state_dict, f'autovc_best.ckpt')\n","torch.save(solver.G.state_dict(), f'autovc_latest.ckpt')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"w-8_8oRwu3SD"},"source":["# Create your own testing corpus"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import pyaudio\n","# import wave\n","# import numpy as np\n","\n","# RECORD_SECONDS = 600  # Record time (seconds)\n","# WAVE_OUTPUT_FILENAME = \"test_wavs/t1/recordedFile_001.wav\"  # Filename\n","# FORMAT = pyaudio.paInt16\n","# CHANNELS = 1\n","# RATE = 22050\n","# CHUNK = 512\n","# device_index = 2\n","# audio = pyaudio.PyAudio()\n","\n","# print(\"----------------------record device list---------------------\")\n","# info = audio.get_host_api_info_by_index(0)\n","# numdevices = info.get('deviceCount')\n","# for i in range(0, numdevices):\n","#     if audio.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels') > 0:\n","#         print(\"Input Device id \", i, \" - \", audio.get_device_info_by_host_api_device_index(0, i).get('name'))\n","\n","# print(\"-------------------------------------------------------------\")\n","\n","# index = int(input(\"Enter recording device index: \"))\n","# print(\"Recording via index \" + str(index))\n","\n","# stream = audio.open(format=FORMAT, channels=CHANNELS,\n","#                     rate=RATE, input=True, input_device_index=index,\n","#                     frames_per_buffer=CHUNK)\n","# print(\"Recording started\")\n","# record_frames = []\n","\n","# for i in range((RATE * (RECORD_SECONDS + 1)) // CHUNK):\n","#     data = stream.read(CHUNK)\n","#     record_frames.append(data)\n","\n","# print(\"Recording stopped\")\n","\n","# stream.stop_stream()\n","# stream.close()\n","# audio.terminate()\n","\n","# # Amplify the audio level\n","# audio_data = np.frombuffer(b''.join(record_frames), dtype=np.int16)\n","# max_amplitude = np.max(np.abs(audio_data))\n","# target_peak_level =  np.iinfo(np.int16).max\n","# gain_factor = target_peak_level / max_amplitude\n","# amplified_data = audio_data * gain_factor\n","# clipped_data = np.clip(amplified_data, -32768, 32767)\n","# amplified_frames = clipped_data.astype(np.int16).tobytes()\n","\n","# waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n","# waveFile.setnchannels(CHANNELS)\n","# waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n","# waveFile.setframerate(RATE)\n","# waveFile.writeframes(amplified_frames)\n","# waveFile.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from pydub import AudioSegment\n","# from pydub.silence import split_on_silence\n","# from pydub.effects import normalize\n","# import numpy as np\n","# import os\n","# from tqdm import tqdm\n","\n","# time_step = 5\n","# wav_name = WAVE_OUTPUT_FILENAME\n","# audio = AudioSegment.from_file(wav_name, \"wav\")\n","# audio_time = len(audio)\n","# cut_parameters = np.arange(time_step, audio_time / 1000, time_step)\n","# start_time = int(0)\n","\n","# print(f'num_uttrs: {len(cut_parameters)}')\n","\n","# for t in tqdm(cut_parameters):\n","#     stop_time = int(t * 1000)\n","#     audio_chunk = audio[start_time:stop_time]\n","\n","#     # Remove empty spaces\n","#     audio_chunks = split_on_silence(\n","#         audio_chunk,\n","#         min_silence_len=500,\n","#         silence_thresh=-50\n","#     )\n","\n","#     # Concatenate the non-silent chunks\n","#     audio_chunk = sum(audio_chunks)\n","\n","#     # Remove sonic boom\n","#     fade_duration = 50  # milliseconds\n","#     audio_chunk = audio_chunk.fade_in(fade_duration).fade_out(fade_duration)\n","\n","#     # Enhance SNR and sound amplitude level\n","#     audio_chunk = normalize(audio_chunk)\n","\n","#     audio_chunk.export(f\"{wav_name}-{t // time_step}.wav\", format=\"wav\")\n","#     start_time = stop_time\n","#     # print(f'done -- {wav_name}-{t // time_step}')\n","\n","# os.remove(wav_name)  # Remove original file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ! python make_spec.py --rootDir \"./test_wavs\" --targetDir \"./test_spmel\"\n","# ! python make_d_vector.py --num_uttrs 120 --rootDir \"./test_spmel\" --model \"./3000000-BL.ckpt\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","train_pkl_path = 'spmel/train.pkl'\n","with open(train_pkl_path, 'rb') as pickle_file:\n","    speakers = pickle.load(pickle_file)\n","\n","for idx in range(len(speakers)):\n","    print(f'{idx} --- {speakers[idx][0]}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import IPython.display as ipd\n","import pickle\n","import torch\n","import numpy as np\n","from model_vc import Generator\n","import noisereduce as nr\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'device: {device}')\n","\n","G = Generator(config.dim_neck, config.dim_emb, config.dim_pre, config.freq).eval().to(device)\n","G.load_state_dict(torch.load('models/autovc_378000_32.ckpt')) # TODO: change to your model\n","org_metadata = pickle.load(open('spmel/train.pkl', \"rb\")) # TODO: your from-corpus\n","trg_metadata = pickle.load(open('spmel/train.pkl', \"rb\")) # TODO: your to-corpus\n","\n","source = 0 # TODO: from-speaker\n","target = 1 # TODO: to-speaker\n","sr = 22050\n","uttr = np.load(\"spmel/p232/p232_003.npy\") # TODO: from-utterance\n","uttr_len = uttr.shape[0]\n","n_pad = config.len_crop - uttr.shape[0] % config.len_crop\n","uttr = np.concatenate((uttr, np.zeros((n_pad, 80))))\n","uttr = uttr.reshape([-1, config.len_crop, 80]) # slice the audio into chunks\n","\n","emb_org = torch.from_numpy(np.expand_dims(org_metadata[source][1],axis=0)).to(device)\n","emb_trg = torch.from_numpy( np.expand_dims(trg_metadata[target][1],axis=0)).to(device)\n","\n","uttr_collector = np.zeros((0,80))\n","\n","for i in range(uttr.shape[0]):\n","    uttr_trg = torch.from_numpy( np.expand_dims(np.squeeze(uttr[i,:]),axis=0)).float().to(device)\n","    with torch.no_grad():\n","        _, x_identic_psnt, _ = G(uttr_trg, emb_org, emb_trg)\n","        uttr_collector = np.concatenate([uttr_collector, x_identic_psnt[0, 0, :, :].cpu().numpy()], axis=0)\n","\n","# To　Waveform\n","from interface import *\n","uttr_collector = uttr_collector[:uttr_len]\n","vocoder = MelVocoder(path=None, github=True)\n","audio = np.squeeze(vocoder.inverse(torch.from_numpy(np.expand_dims(uttr_collector.T,axis=0)).float()).cpu().numpy())\n","\n","# Enhance audio quality\n","# mean, std = np.mean(audio), np.std(audio)\n","# audio = (audio - mean) / std\n","\n","# reduced_noise = nr.reduce_noise(y=audio, sr=sr, hop_length=256, n_fft=1024, n_jobs=-1)\n","audio = ipd.Audio(audio, rate = sr)\n","\n","with open('inference_003.wav', 'wb') as f: # saved file name\n","    f.write(audio.data)\n","    \n","audio"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"vst","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
